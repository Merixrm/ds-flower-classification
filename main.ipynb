{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76dc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory growth (prevents TensorFlow from pre-allocating all VRAM)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "print(\"Num GPUs Available:\", len(gpus))\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Built with GPU support:\", tf.test.is_built_with_gpu_support())\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable memory growth (prevents TensorFlow from pre-allocating all VRAM)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "print(\"Num of GPUs\", len(gpus))\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef24ee7",
   "metadata": {},
   "source": [
    "**load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27047eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDir = os.getcwd()\n",
    "\n",
    "Train_filePath = os.path.join(currentDir, \"Flower Classification Dataset/train\")\n",
    "Test_filePath = os.path.join(currentDir, \"Flower Classification Dataset/test\")\n",
    "Validation_filePath = os.path.join(currentDir, \"Flower Classification Dataset/valid\")\n",
    "\n",
    "Train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    Train_filePath,\n",
    "    labels = 'inferred',\n",
    "    image_size = (224,224),\n",
    "    batch_size = 102,\n",
    "    #validation_split = 0\n",
    ")\n",
    "\n",
    "Test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    Test_filePath,\n",
    "    labels = 'inferred',\n",
    "    image_size = (224,224),\n",
    "    batch_size = 102,\n",
    "    #validation_split = 0\n",
    ")\n",
    "\n",
    "Validation_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    Validation_filePath,\n",
    "    labels = 'inferred',\n",
    "    image_size = (224,224),\n",
    "    batch_size = 102,\n",
    "    #validation_split = 0\n",
    ")\n",
    "\n",
    "#Split the dataset into input and target\n",
    "# Input_validation= Validation_data.map(lambda image, lable: image)\n",
    "# Target_validation= Validation_data.map(lambda image, lable: lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561332c",
   "metadata": {},
   "source": [
    "**Outline the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 50176\n",
    "output_size = 102\n",
    "hidden_layer_size = 128 #TODO: figure out the exact value\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # ReScale pixel values in the band [0, 1] \n",
    "    tf.keras.Input(shape=(224, 224, 3)),\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "\n",
    "    # Augmentation (Adding randomization to the images)\n",
    "    # ONLY USED IN TRAINING\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "    \n",
    "    # Conv 1\n",
    "    tf.keras.layers.Conv2D(\n",
    "        32, \n",
    "        (3,3), \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        padding='same'\n",
    "    ), #TODO: Check if the combination of L1 and L2 works better for all layers\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    # Conv 2\n",
    "    tf.keras.layers.Conv2D(\n",
    "        64, \n",
    "        (3,3), \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        padding='same'\n",
    "    ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    # Conv 3\n",
    "    tf.keras.layers.Conv2D(\n",
    "        64, \n",
    "        (3,3), \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        padding='same'\n",
    "    ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    # Conv 4\n",
    "    tf.keras.layers.Conv2D(\n",
    "        128, \n",
    "        (3,3), \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        padding='same'\n",
    "    ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    # Flattened Inputs\n",
    "    tf.keras.layers.GlobalAveragePooling2D(), # Better than normal Flattening, reduces the dimensionality of feature maps\n",
    "        #tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # Dense Layers (Fully connected layers (FC Layers))\n",
    "    tf.keras.layers.Dense(\n",
    "        hidden_layer_size, \n",
    "        activation = 'relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    ),\n",
    "    #TODO: Check if dropping neurons actually prevents overfitting \n",
    "    #TODO: We can test other techniques besides dropping \n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(        \n",
    "        hidden_layer_size, \n",
    "        activation = 'relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    ),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    # Didn't specify \"softmax\" activation function in the last layer \n",
    "    # to use the right order of opertions for loss calculations\n",
    "    tf.keras.layers.Dense(output_size) \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e2c968",
   "metadata": {},
   "source": [
    "**Choose the optimizer and the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c07d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # Internally applies \"softmax\" if \"from_logits = True\"\n",
    "\n",
    "#TODO: Add early stopping\n",
    "model.compile(optimizer= 'adam' ,loss=lossFunction, metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f7b33",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607afef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num= 10\n",
    "\n",
    "model.fit(Train_data, validation_data= Validation_data, epochs= epoch_num, verbose= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503da27",
   "metadata": {},
   "source": [
    "**Evaluate the model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_accuracy = model.evaluate(Test_data)\n",
    "evaluation = model.evaluate(Test_data)\n",
    "print(f\"Evaluation: {evaluation}\")\n",
    "# print(f\"Test accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
